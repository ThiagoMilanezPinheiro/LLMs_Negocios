# ğŸ—ï¸ Arquitetura do Sistema - SafeBank Chatbot

## ğŸ“ VisÃ£o Geral

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   UsuÃ¡rio   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ Interface Web
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Streamlit UI           â”‚
â”‚  (agent_app.py)             â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ Input/History
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Chat Flow Handler         â”‚
â”‚  - ValidaÃ§Ã£o entrada        â”‚
â”‚  - Gerenciamento histÃ³rico  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ Query
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   RAG Pipeline              â”‚
â”‚  1. Contextualize Question  â”‚
â”‚  2. Retrieve Documents      â”‚
â”‚  3. Generate Answer         â”‚
â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
   â”‚                   â”‚
   â”‚ Reformulated Q    â”‚ Context
   â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Groq LLM  â”‚    â”‚    FAISS     â”‚
â”‚  (ChatGPT) â”‚    â”‚ Vector Store â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â”‚ Embeddings
                         â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ HuggingFace  â”‚
                  â”‚  Embeddings  â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ§© Componentes Principais

### 1. **Interface de UsuÃ¡rio (Streamlit)**
- **Responsabilidade:** ApresentaÃ§Ã£o e interaÃ§Ã£o
- **Tecnologia:** Streamlit
- **Funcionalidades:**
  - Chat interface
  - HistÃ³rico de mensagens
  - Debug panels (expansÃ­vel)
  - Feedback visual (spinners, errors)

### 2. **Chat Flow Handler**
- **Responsabilidade:** OrquestraÃ§Ã£o de conversas
- **FunÃ§Ãµes principais:**
  - `chat_llm_flow()`: Gerencia fluxo completo
  - ValidaÃ§Ã£o de entrada (tamanho, conteÃºdo)
  - ManutenÃ§Ã£o de histÃ³rico
  - Tratamento de erros

### 3. **RAG Pipeline**
- **Responsabilidade:** RecuperaÃ§Ã£o e geraÃ§Ã£o
- **Componentes:**
  - **Contextualize Chain:** Reformula pergunta com histÃ³rico
  - **Retriever:** Busca documentos relevantes
  - **Answer Chain:** Gera resposta final

### 4. **Vector Store (FAISS)**
- **Responsabilidade:** Busca semÃ¢ntica
- **Tecnologia:** FAISS (Facebook AI Similarity Search)
- **Processo:**
  1. Documentos â†’ Chunks (1000 chars, overlap 200)
  2. Chunks â†’ Embeddings (BAAI/bge-m3)
  3. Embeddings â†’ Ãndice FAISS
  4. Query â†’ MMR search â†’ Top-3 chunks

### 5. **LLM Provider (Groq)**
- **Responsabilidade:** GeraÃ§Ã£o de linguagem natural
- **Modelo:** deepseek-r1-distill-llama-70b
- **ParÃ¢metros:**
  - Temperature: 0.7
  - Max retries: 2
  - Timeout: 60s

## ğŸ”„ Fluxo de Dados

### InicializaÃ§Ã£o
```python
1. Load .env variables
2. Validate GROQ_API_KEY
3. Initialize LLM (ChatGroq)
4. On first query:
   - Load PDFs from content/
   - Create chunks
   - Generate embeddings
   - Build FAISS index
   - Initialize retriever
```

### Query Processing
```python
1. User input â†’ Validation (length, empty)
2. Add HumanMessage to history
3. Contextualize question with history
4. Retrieve relevant chunks (MMR, k=3)
5. Build context from chunks (max 4000 chars)
6. Generate answer with LLM
7. Add AIMessage to history
8. Display answer + debug info
```

## ğŸ“Š Dados e Estado

### Session State (Streamlit)
```python
st.session_state = {
    "chat_history": [
        AIMessage("OlÃ¡..."),
        HumanMessage("Pergunta 1"),
        AIMessage("Resposta 1"),
        ...
    ],
    "retriever": <FAISS retriever object>
}
```

### Estrutura de Mensagens
```python
# LangChain message types
HumanMessage(content="texto do usuÃ¡rio")
AIMessage(content="texto do assistente")
```

### RAG Result
```python
{
    "answer": "Resposta final do LLM",
    "reformulated_question": "Pergunta contextualizada",
    "similarity_used": None,
    "used_chunks_preview": ["chunk1...", "chunk2..."]
}
```

## ğŸ” SeguranÃ§a

### Camadas de ProteÃ§Ã£o
1. **ValidaÃ§Ã£o de Entrada:**
   - Limite de 5000 caracteres
   - SanitizaÃ§Ã£o bÃ¡sica
   - DetecÃ§Ã£o de input vazio

2. **API Key Management:**
   - Nunca hardcoded
   - Carregada via .env
   - ValidaÃ§Ã£o na inicializaÃ§Ã£o

3. **Error Handling:**
   - Try-catch em todas operaÃ§Ãµes crÃ­ticas
   - Mensagens de erro amigÃ¡veis
   - Logging detalhado para debug

4. **Rate Limiting:**
   - Timeout de 60s por request
   - Max retries: 2

## ğŸ“ˆ Performance

### OtimizaÃ§Ãµes
- **Chunk Size:** 1000 chars (balance context/speed)
- **Overlap:** 200 chars (mantÃ©m continuidade)
- **MMR Search:** Diversidade nos resultados
- **Context Limit:** 4000 chars (evita timeouts)
- **Index Persistence:** FAISS salvo em disco

### MÃ©tricas Esperadas
- **Tempo de inicializaÃ§Ã£o:** 10-30s (depende dos PDFs)
- **Tempo de resposta:** 2-5s por query
- **MemÃ³ria:** ~500MB-1GB (depende do modelo embedding)

## ğŸ§ª Testabilidade

### Pontos de Teste
1. **Unidade:**
   - `extract_text_pdf()`: Validar extraÃ§Ã£o
   - `load_llm()`: Validar inicializaÃ§Ã£o
   - ValidaÃ§Ãµes de entrada

2. **IntegraÃ§Ã£o:**
   - Pipeline RAG completo
   - FAISS indexing/retrieval
   - LLM responses

3. **E2E:**
   - User flow completo
   - MÃºltiplas perguntas com contexto

## ğŸ”§ Configurabilidade

### VariÃ¡veis de Ambiente
```env
GROQ_API_KEY         # Chave da API
GROQ_MODEL_ID        # Modelo a usar
GROQ_TEMPERATURE     # Criatividade (0-1)
CONTENT_PATH         # DiretÃ³rio dos PDFs
EMBEDDING_MODEL      # Modelo de embeddings
FAISS_INDEX_DIR      # DiretÃ³rio do Ã­ndice
```

### HyperparÃ¢metros
```python
# Text Splitting
chunk_size = 1000
chunk_overlap = 200

# Retrieval
search_type = "mmr"
k = 3                # Top chunks
fetch_k = 4          # Pool para MMR

# Context
max_context_len = 4000  # chars
```

## ğŸš€ Escalabilidade

### LimitaÃ§Ãµes Atuais
- Single-user session state
- In-memory FAISS index
- Sem cache de queries

### Melhorias PossÃ­veis
1. **Multi-user:** Redis para session state
2. **Scaling:** Load balancer + mÃºltiplas instÃ¢ncias
3. **Cache:** Redis/Memcached para queries frequentes
4. **Database:** PostgreSQL com pgvector
5. **Monitoring:** Prometheus + Grafana

## ğŸ¯ PadrÃµes de Design

- **Chain of Responsibility:** RAG pipeline
- **Singleton:** LLM instance
- **Repository:** Vector store abstraction
- **Observer:** Streamlit reactive updates
- **Factory:** LLM loader

---

*Ãšltima atualizaÃ§Ã£o: Novembro 2025*
